{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, output_shape, input_shape, activation=\"relu\"):\n",
    "        w_init = tf.keras.initializers.Zeros()\n",
    "        self.weights = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_shape, output_shape), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.weights = self.weights + np.random.normal(size=(input_shape, output_shape), scale=0.3)\n",
    "\n",
    "        b_init = tf.keras.initializers.Zeros()\n",
    "        self.biases = tf.Variable(\n",
    "            initial_value=b_init(shape=(output_shape,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "        self.output_shape = output_shape\n",
    "        self.input_shape = input_shape\n",
    "        self.size = self.weights.shape\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            self.activation = tf.keras.activations.relu\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = tf.keras.activations.tanh\n",
    "        \n",
    "    def add_weights(self, weights):\n",
    "        self.weights = self.weights + weights\n",
    "        \n",
    "    def add_biases(self, biases):\n",
    "        self.biases = self.biases + biases\n",
    "    \n",
    "    def extend(self):\n",
    "        self.weights = np.append(self.weights, np.random.normal(size=(self.input_shape,1)), axis=1)\n",
    "        self.biases = np.append(self.biases, 0)        \n",
    "        self.output_shape += 1\n",
    "        self.size = self.weights.shape\n",
    "    \n",
    "    def extend_input(self):\n",
    "        self.weights = np.append(self.weights, np.random.normal(size=(1,self.output_shape)), axis=0)\n",
    "        self.input_shape += 1\n",
    "        self.size = self.weights.shape\n",
    "        \n",
    "    def decrease(self):\n",
    "        self.weights = np.delete(self.weights, -1, axis=1)\n",
    "        self.biases = np.delete(self.biases, -1)        \n",
    "        self.output_shape -= 1\n",
    "        self.size = self.weights.shape\n",
    "    \n",
    "    def decrease_input(self):\n",
    "        self.weights = np.delete(self.weights, -1, axis=0)\n",
    "        self.input_shape -= 1\n",
    "        self.size = self.weights.shape\n",
    "    \n",
    "    def change_output_size(self, size):\n",
    "        while self.output_shape != size:\n",
    "            if self.output_shape > size:\n",
    "                self.decrease()\n",
    "            elif self.output_shape < size:\n",
    "                self.extend()\n",
    "                \n",
    "    def call(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.weights) + self.biases)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Layer: [shape=\" + str(self.weights.shape) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, input_shape):\n",
    "        self.layers = []\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def add_layer(self, size, weights=None, biases=None, activation=\"relu\"):\n",
    "        if len(self.layers) == 0:\n",
    "            self.layers.append(\n",
    "                Layer(\n",
    "                    size,\n",
    "                    self.input_shape,  \n",
    "                    activation=activation, \n",
    "                    #weights=weights, \n",
    "                    #biases=biases\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.layers.append(\n",
    "                Layer(\n",
    "                    size, \n",
    "                    self.layers[-1].output_shape, \n",
    "                    activation=activation,\n",
    "                    #weights=weights, \n",
    "                    #biases=biases\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    def predict(self, sample):\n",
    "        result = sample\n",
    "        for layer in self.layers:\n",
    "            result = layer.call(result)\n",
    "        return np.array(result)\n",
    "    \n",
    "    \n",
    "    def mutate_weights(self, intensity=0.05):\n",
    "        for layer in self.layers:\n",
    "            layer.add_weights(np.random.normal(scale=intensity, size=layer.size))\n",
    "            layer.add_biases(np.random.normal(scale=intensity, size=layer.output_shape))\n",
    "            \n",
    "    def mutate_layers(self, propa=0.5):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            if np.random.choice([True, False], p=[propa,1-propa]):\n",
    "                self.layers[i].extend()\n",
    "                self.layers[i+1].extend_input()\n",
    "            elif np.random.choice([True, False], p=[propa,1-propa]):\n",
    "                if self.layers[i].output_shape > 3:\n",
    "                    self.layers[i].decrease()\n",
    "                    self.layers[i+1].decrease_input()\n",
    "        \n",
    "    def mutate_topology(self, propa=0.1, activation=\"tanh\"):\n",
    "        choice = np.random.choice([0, 1], p=[propa,1-propa])\n",
    "        # Add a Layer\n",
    "        if choice == 0:\n",
    "            if len(self.layers) == 1:\n",
    "                index = 1\n",
    "                layer = Layer(input_shape=self.layers[0].output_shape, output_shape=self.layers[0].output_shape, activation=activation)\n",
    "            else:\n",
    "                index = np.random.randint(1, len(self.layers))\n",
    "                layer = Layer(input_shape=self.layers[index-1].output_shape, output_shape=self.layers[index].input_shape, activation=activation)\n",
    "\n",
    "            self.layers.insert(index, layer)\n",
    "        # Remove a layer\n",
    "        elif choice == 1:\n",
    "            if len(self.layers) > 2:\n",
    "                index = np.random.randint(1, len(self.layers) - 1)\n",
    "                self.layers[index-1].change_output_size(self.layers[index+1].input_shape)\n",
    "                del self.layers[index]\n",
    "            \n",
    "            \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Network: Layers= \\n\" + str(self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: Layers= \n",
      "[Layer: [shape=(2, 5)], Layer: [shape=(5, 5)], Layer: [shape=(5, 8)], Layer: [shape=(8, 7)], Layer: [shape=(7, 3)], Layer: [shape=(3, 3)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.0796938e-05, 0.0000000e+00, 0.0000000e+00],\n",
       "       [6.0796938e-05, 0.0000000e+00, 0.0000000e+00],\n",
       "       [6.0796938e-05, 0.0000000e+00, 0.0000000e+00],\n",
       "       [6.0796938e-05, 0.0000000e+00, 0.0000000e+00],\n",
       "       [6.0796938e-05, 0.0000000e+00, 0.0000000e+00],\n",
       "       [6.0796938e-05, 0.0000000e+00, 0.0000000e+00],\n",
       "       [6.0796938e-05, 0.0000000e+00, 0.0000000e+00],\n",
       "       [6.0796938e-05, 0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(input_shape=2)\n",
    "net.add_layer(4)\n",
    "\n",
    "net.add_layer(7)\n",
    "net.add_layer(6)\n",
    "net.add_layer(2)\n",
    "net.add_layer(3)\n",
    "\n",
    "\n",
    "#net.mutate_layers(propa=0.1)\n",
    "net.mutate_topology(propa=1)\n",
    "net.mutate_layers(propa=1)\n",
    "\n",
    "print(net)\n",
    "net.predict(np.ones((8,2)).astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class NetworkPool():\n",
    "    def __init__(self, input_shape, output_shape, population_size=100):\n",
    "        self.networks = []\n",
    "        for _ in range(population_size):\n",
    "            \n",
    "            net = Network(input_shape=input_shape)\n",
    "            #net.add_layer(128, activation=\"relu\")\n",
    "            net.add_layer(output_shape, activation=\"relu\")\n",
    "            \n",
    "            self.networks.append(net)            \n",
    "        #self.networks = np.array(self.networks)\n",
    "            \n",
    "    \n",
    "    def fit(self, data, epochs=100, num_survivors=10, num_children=10, batch_size=32, loss=tf.keras.losses.mean_squared_error):\n",
    "        X, y = data\n",
    "        \n",
    "        batch_start = 0\n",
    "        \n",
    "        t = trange(epochs, desc='Loss', leave=True)\n",
    "        for _ in t:            \n",
    "            batch_X = np.array(X[batch_start:np.min([batch_start+batch_size, len(X)])]).astype('float32')\n",
    "            batch_y = np.array(y[batch_start:np.min([batch_start+batch_size, len(y)])]).astype('float32')\n",
    "            \n",
    "            batch_start = (batch_start + batch_size) % len(X)\n",
    "            \n",
    "            losses = []\n",
    "            for network in self.networks:\n",
    "                losses.append(loss(batch_y, network.predict(batch_X)))\n",
    "            \n",
    "            losses = np.array(losses)#.reshape(len(self.networks), -1).astype('float32')\n",
    "            \n",
    "            #print(loss(predictions, [batch_y]))\n",
    "            #print(loss(predictions, np.repeat(batch_y, 1))[0])\n",
    "            #print(tf.math.reduce_mean(loss(predictions, np.repeat(batch_y, len(predictions))), axis=-1)[0])\n",
    "            #losses = tf.math.reduce_mean(loss(predictions, batch_y), axis=-1)\n",
    "            t.set_description(\"Population: loss_min={:.4f}, loss_avg={:.4f}\".format(np.min(losses), np.mean(losses)))\n",
    "\n",
    "            idx = np.argsort(losses)\n",
    "\n",
    "            survivors = [self.networks[x] for x in idx[:num_survivors]] #self.networks[idx[:num_survivors]]\n",
    "            \n",
    "            self.best_network = self.networks[idx[0]]\n",
    "            \n",
    "            \n",
    "            # Mutate all surviors and use them as the new networks\n",
    "            # The best network survives\n",
    "            self.networks = [self.best_network]\n",
    "            for survivor1 in survivors:\n",
    "                #for survivor2 in survivors:\n",
    "                for _ in range(num_children):\n",
    "                    #child = survivor1.crossbreed(survivor2, random_mutations=True)\n",
    "                    child = copy.deepcopy(survivor1)\n",
    "                    child.mutate_topology() \n",
    "                    child.mutate_layers()\n",
    "                    child.mutate_weights()\n",
    "                    self.networks.append(child)\n",
    "            \n",
    "            #self.networks = np.array(self.networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0], [0,1], [1,0], [1,1]]).astype(np.float32)\n",
    "y = np.array([[0.], [1.], [1.], [0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Population: loss_min=0.0000, loss_avg=0.1017: 100%|██████████| 100/100 [00:11<00:00,  8.81it/s]\n"
     ]
    }
   ],
   "source": [
    "loss=tf.keras.losses.MeanSquaredError()#reduction=tf.keras.losses.Reduction.NONE)\n",
    "pool = NetworkPool(input_shape=2, output_shape=1, population_size=200)\n",
    "pool.fit((X,y), epochs=100, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00142315]\n",
      " [ 0.99984443]\n",
      " [ 0.99959326]\n",
      " [-0.00163677]]\n",
      "Network: Layers= \n",
      "[Layer: [shape=(2, 9)], Layer: [shape=(9, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(pool.best_network.predict(X))\n",
    "print(pool.best_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "y_train_onehot = np.zeros((y_train.size, y_train.max()+1))\n",
    "y_train_onehot[np.arange(y_train.size),y_train] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1974863349995617e-18"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = NetworkPool(input_shape=784,output_shape=10, population_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Population: loss_min=1.6739, loss_avg=2.3985: 100%|██████████| 100/100 [00:49<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "loss=tf.keras.losses.CategoricalCrossentropy()\n",
    "pool.fit((x_train,y_train_onehot), epochs=100, batch_size=2000, num_survivors=10, num_children=10, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network: Layers= \n",
       "[Layer: [shape=(784, 53)], Layer: [shape=(53, 10)]]"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.best_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 0 1]\n",
      "[7 2 1 ... 4 5 6]\n",
      "tf.Tensor(23, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "index = [2547,19910,12233,54220]\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#print(pool.best_network.predict([x_train[index]]))\n",
    "print(np.argmax(pool.best_network.predict(x_test), axis=1))\n",
    "print(y_test)\n",
    "print(tf.keras.losses.mean_squared_error(np.argmax(pool.best_network.predict(x_test), axis=1).astype('int32'), y_test.astype('int32')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
       "array([[4., 4.],\n",
       "       [4., 4.]])>"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(np.ones((2,4)), np.ones((4,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]]\n",
      "[[0.1, 1.2, 0.1], [0.1, 1.1, 1.1]]\n",
      "0.44587487\n"
     ]
    }
   ],
   "source": [
    "loss=tf.keras.losses.CategoricalCrossentropy()#reduction=tf.keras.losses.Reduction.NONE)\n",
    "y_true = np.repeat([[0,1,0]], 2, axis=0)#, [0,1,1]]\n",
    "y_pred = [[0.1,1.2,0.1], [0.1,1.1,1.1]]\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "print(loss(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.539699"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_onehot = np.zeros((y_test.size, y_test.max()+1))\n",
    "y_test_onehot[np.arange(y_test.size),y_test] = 1\n",
    "\n",
    "y_true = y_test_onehot#np.repeat(y_test_onehot, len(x_test), axis=-1)\n",
    "y_pred = pool.best_network.predict(x_test)\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
